{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Titanic Survival Prediction\n",
    "This ML Model will be used to predict passenger survival based on the dataset from Kaggle (https://www.kaggle.com/competitions/titanic). In this approach, we will be using a Tensorflow/Keras DNN to perform predictions on the test.csv file after training with the supplied train.csv file."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Importing Libraries/Modules\n",
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Preprocessing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First, lets open the csv file and preview the data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>PassengerId</th>\n",
       "      <th>Survived</th>\n",
       "      <th>Pclass</th>\n",
       "      <th>Name</th>\n",
       "      <th>Sex</th>\n",
       "      <th>Age</th>\n",
       "      <th>SibSp</th>\n",
       "      <th>Parch</th>\n",
       "      <th>Ticket</th>\n",
       "      <th>Fare</th>\n",
       "      <th>Cabin</th>\n",
       "      <th>Embarked</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "      <td>Braund, Mr. Owen Harris</td>\n",
       "      <td>male</td>\n",
       "      <td>22.0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>A/5 21171</td>\n",
       "      <td>7.2500</td>\n",
       "      <td>NaN</td>\n",
       "      <td>S</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>Cumings, Mrs. John Bradley (Florence Briggs Th...</td>\n",
       "      <td>female</td>\n",
       "      <td>38.0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>PC 17599</td>\n",
       "      <td>71.2833</td>\n",
       "      <td>C85</td>\n",
       "      <td>C</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "      <td>Heikkinen, Miss. Laina</td>\n",
       "      <td>female</td>\n",
       "      <td>26.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>STON/O2. 3101282</td>\n",
       "      <td>7.9250</td>\n",
       "      <td>NaN</td>\n",
       "      <td>S</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>4</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>Futrelle, Mrs. Jacques Heath (Lily May Peel)</td>\n",
       "      <td>female</td>\n",
       "      <td>35.0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>113803</td>\n",
       "      <td>53.1000</td>\n",
       "      <td>C123</td>\n",
       "      <td>S</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>5</td>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "      <td>Allen, Mr. William Henry</td>\n",
       "      <td>male</td>\n",
       "      <td>35.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>373450</td>\n",
       "      <td>8.0500</td>\n",
       "      <td>NaN</td>\n",
       "      <td>S</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>886</th>\n",
       "      <td>887</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>Montvila, Rev. Juozas</td>\n",
       "      <td>male</td>\n",
       "      <td>27.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>211536</td>\n",
       "      <td>13.0000</td>\n",
       "      <td>NaN</td>\n",
       "      <td>S</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>887</th>\n",
       "      <td>888</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>Graham, Miss. Margaret Edith</td>\n",
       "      <td>female</td>\n",
       "      <td>19.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>112053</td>\n",
       "      <td>30.0000</td>\n",
       "      <td>B42</td>\n",
       "      <td>S</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>888</th>\n",
       "      <td>889</td>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "      <td>Johnston, Miss. Catherine Helen \"Carrie\"</td>\n",
       "      <td>female</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>W./C. 6607</td>\n",
       "      <td>23.4500</td>\n",
       "      <td>NaN</td>\n",
       "      <td>S</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>889</th>\n",
       "      <td>890</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>Behr, Mr. Karl Howell</td>\n",
       "      <td>male</td>\n",
       "      <td>26.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>111369</td>\n",
       "      <td>30.0000</td>\n",
       "      <td>C148</td>\n",
       "      <td>C</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>890</th>\n",
       "      <td>891</td>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "      <td>Dooley, Mr. Patrick</td>\n",
       "      <td>male</td>\n",
       "      <td>32.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>370376</td>\n",
       "      <td>7.7500</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Q</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>891 rows × 12 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "     PassengerId  Survived  Pclass  \\\n",
       "0              1         0       3   \n",
       "1              2         1       1   \n",
       "2              3         1       3   \n",
       "3              4         1       1   \n",
       "4              5         0       3   \n",
       "..           ...       ...     ...   \n",
       "886          887         0       2   \n",
       "887          888         1       1   \n",
       "888          889         0       3   \n",
       "889          890         1       1   \n",
       "890          891         0       3   \n",
       "\n",
       "                                                  Name     Sex   Age  SibSp  \\\n",
       "0                              Braund, Mr. Owen Harris    male  22.0      1   \n",
       "1    Cumings, Mrs. John Bradley (Florence Briggs Th...  female  38.0      1   \n",
       "2                               Heikkinen, Miss. Laina  female  26.0      0   \n",
       "3         Futrelle, Mrs. Jacques Heath (Lily May Peel)  female  35.0      1   \n",
       "4                             Allen, Mr. William Henry    male  35.0      0   \n",
       "..                                                 ...     ...   ...    ...   \n",
       "886                              Montvila, Rev. Juozas    male  27.0      0   \n",
       "887                       Graham, Miss. Margaret Edith  female  19.0      0   \n",
       "888           Johnston, Miss. Catherine Helen \"Carrie\"  female   NaN      1   \n",
       "889                              Behr, Mr. Karl Howell    male  26.0      0   \n",
       "890                                Dooley, Mr. Patrick    male  32.0      0   \n",
       "\n",
       "     Parch            Ticket     Fare Cabin Embarked  \n",
       "0        0         A/5 21171   7.2500   NaN        S  \n",
       "1        0          PC 17599  71.2833   C85        C  \n",
       "2        0  STON/O2. 3101282   7.9250   NaN        S  \n",
       "3        0            113803  53.1000  C123        S  \n",
       "4        0            373450   8.0500   NaN        S  \n",
       "..     ...               ...      ...   ...      ...  \n",
       "886      0            211536  13.0000   NaN        S  \n",
       "887      0            112053  30.0000   B42        S  \n",
       "888      2        W./C. 6607  23.4500   NaN        S  \n",
       "889      0            111369  30.0000  C148        C  \n",
       "890      0            370376   7.7500   NaN        Q  \n",
       "\n",
       "[891 rows x 12 columns]"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#CSV Filepath\n",
    "train_data_path = 'Data/train.csv'\n",
    "test_data_path = 'Data/test.csv'\n",
    "\n",
    "train_data = pd.read_csv(train_data_path, delimiter=',')\n",
    "train_data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There are 890 rows of training data. As seen from above, the data requires cleaning & preprocessing before it can be fed into a Neural Network. \n",
    "\n",
    "First, we will drop the columns which are not relevant to training:\n",
    "- PassengerId: We do not require the Passenger Id for training our model since it is just an index.\n",
    "- Name: The name of the passenger is not required as it has no impact on the result.\n",
    "- Ticket: This is just the ticket number, again this should not have an impact on the result.\n",
    "- Cabin: This may have an impact on the result, however there are a large number of rows without a cabin value (only 204 out of 890) so this will be discarded.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Survived</th>\n",
       "      <th>Pclass</th>\n",
       "      <th>Sex</th>\n",
       "      <th>Age</th>\n",
       "      <th>SibSp</th>\n",
       "      <th>Parch</th>\n",
       "      <th>Fare</th>\n",
       "      <th>Embarked</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "      <td>male</td>\n",
       "      <td>22.0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>7.2500</td>\n",
       "      <td>S</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>female</td>\n",
       "      <td>38.0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>71.2833</td>\n",
       "      <td>C</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "      <td>female</td>\n",
       "      <td>26.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>7.9250</td>\n",
       "      <td>S</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>female</td>\n",
       "      <td>35.0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>53.1000</td>\n",
       "      <td>S</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "      <td>male</td>\n",
       "      <td>35.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>8.0500</td>\n",
       "      <td>S</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>886</th>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>male</td>\n",
       "      <td>27.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>13.0000</td>\n",
       "      <td>S</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>887</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>female</td>\n",
       "      <td>19.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>30.0000</td>\n",
       "      <td>S</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>888</th>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "      <td>female</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>23.4500</td>\n",
       "      <td>S</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>889</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>male</td>\n",
       "      <td>26.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>30.0000</td>\n",
       "      <td>C</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>890</th>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "      <td>male</td>\n",
       "      <td>32.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>7.7500</td>\n",
       "      <td>Q</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>891 rows × 8 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "     Survived  Pclass     Sex   Age  SibSp  Parch     Fare Embarked\n",
       "0           0       3    male  22.0      1      0   7.2500        S\n",
       "1           1       1  female  38.0      1      0  71.2833        C\n",
       "2           1       3  female  26.0      0      0   7.9250        S\n",
       "3           1       1  female  35.0      1      0  53.1000        S\n",
       "4           0       3    male  35.0      0      0   8.0500        S\n",
       "..        ...     ...     ...   ...    ...    ...      ...      ...\n",
       "886         0       2    male  27.0      0      0  13.0000        S\n",
       "887         1       1  female  19.0      0      0  30.0000        S\n",
       "888         0       3  female   NaN      1      2  23.4500        S\n",
       "889         1       1    male  26.0      0      0  30.0000        C\n",
       "890         0       3    male  32.0      0      0   7.7500        Q\n",
       "\n",
       "[891 rows x 8 columns]"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_data = train_data.drop(['PassengerId','Name','Ticket','Cabin'],axis=1)\n",
    "train_data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, we need to handle null values for the remaining columns:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Survived      0\n",
       "Pclass        0\n",
       "Sex           0\n",
       "Age         177\n",
       "SibSp         0\n",
       "Parch         0\n",
       "Fare          0\n",
       "Embarked      2\n",
       "dtype: int64"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_data.isnull().sum()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "From the output, it is evident that there are a significant number of rows without age. We have two options here - either discard null rows OR fill in the values using some statistical approaches like mean, mode etc. Since we only have around 800 rows of data, dropping ~170 rows would affect our training, so we will instead fill in with mean.\n",
    "For the other column 'Embarked', we can just drop the rows since there are only 2 rows with missing data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/94/j0q38tb53cddks5lvhm9vjlc0000gn/T/ipykernel_2900/1827510441.py:2: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  train_data['Age'] = train_data['Age'].fillna(train_data['Age'].mean())\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Survived</th>\n",
       "      <th>Pclass</th>\n",
       "      <th>Sex</th>\n",
       "      <th>Age</th>\n",
       "      <th>SibSp</th>\n",
       "      <th>Parch</th>\n",
       "      <th>Fare</th>\n",
       "      <th>Embarked</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "      <td>male</td>\n",
       "      <td>22.000000</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>7.2500</td>\n",
       "      <td>S</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>female</td>\n",
       "      <td>38.000000</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>71.2833</td>\n",
       "      <td>C</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "      <td>female</td>\n",
       "      <td>26.000000</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>7.9250</td>\n",
       "      <td>S</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>female</td>\n",
       "      <td>35.000000</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>53.1000</td>\n",
       "      <td>S</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "      <td>male</td>\n",
       "      <td>35.000000</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>8.0500</td>\n",
       "      <td>S</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>886</th>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>male</td>\n",
       "      <td>27.000000</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>13.0000</td>\n",
       "      <td>S</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>887</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>female</td>\n",
       "      <td>19.000000</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>30.0000</td>\n",
       "      <td>S</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>888</th>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "      <td>female</td>\n",
       "      <td>29.642093</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>23.4500</td>\n",
       "      <td>S</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>889</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>male</td>\n",
       "      <td>26.000000</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>30.0000</td>\n",
       "      <td>C</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>890</th>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "      <td>male</td>\n",
       "      <td>32.000000</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>7.7500</td>\n",
       "      <td>Q</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>889 rows × 8 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "     Survived  Pclass     Sex        Age  SibSp  Parch     Fare Embarked\n",
       "0           0       3    male  22.000000      1      0   7.2500        S\n",
       "1           1       1  female  38.000000      1      0  71.2833        C\n",
       "2           1       3  female  26.000000      0      0   7.9250        S\n",
       "3           1       1  female  35.000000      1      0  53.1000        S\n",
       "4           0       3    male  35.000000      0      0   8.0500        S\n",
       "..        ...     ...     ...        ...    ...    ...      ...      ...\n",
       "886         0       2    male  27.000000      0      0  13.0000        S\n",
       "887         1       1  female  19.000000      0      0  30.0000        S\n",
       "888         0       3  female  29.642093      1      2  23.4500        S\n",
       "889         1       1    male  26.000000      0      0  30.0000        C\n",
       "890         0       3    male  32.000000      0      0   7.7500        Q\n",
       "\n",
       "[889 rows x 8 columns]"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_data = train_data.dropna(subset=['Embarked'])\n",
    "train_data['Age'] = train_data['Age'].fillna(train_data['Age'].mean())\n",
    "train_data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Handling Categorical Values\n",
    "For the columns with categorical values, we will perform one-hot encoding. Note that this is feasible only if the number of categories is not very large, because the cardinality of the dataset will increase. One-hot encoding is done to prevent the model from misinterpreting data (e.g. the model might think a pclass of 3 is greater than a pclass of 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Survived</th>\n",
       "      <th>Age</th>\n",
       "      <th>SibSp</th>\n",
       "      <th>Parch</th>\n",
       "      <th>Fare</th>\n",
       "      <th>female</th>\n",
       "      <th>male</th>\n",
       "      <th>Pclass_1</th>\n",
       "      <th>Pclass_2</th>\n",
       "      <th>Pclass_3</th>\n",
       "      <th>Embarked_C</th>\n",
       "      <th>Embarked_Q</th>\n",
       "      <th>Embarked_S</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>22.000000</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>7.2500</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>38.000000</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>71.2833</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1</td>\n",
       "      <td>26.000000</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>7.9250</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1</td>\n",
       "      <td>35.000000</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>53.1000</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0</td>\n",
       "      <td>35.000000</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>8.0500</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>886</th>\n",
       "      <td>0</td>\n",
       "      <td>27.000000</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>13.0000</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>887</th>\n",
       "      <td>1</td>\n",
       "      <td>19.000000</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>30.0000</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>888</th>\n",
       "      <td>0</td>\n",
       "      <td>29.642093</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>23.4500</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>889</th>\n",
       "      <td>1</td>\n",
       "      <td>26.000000</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>30.0000</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>890</th>\n",
       "      <td>0</td>\n",
       "      <td>32.000000</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>7.7500</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>889 rows × 13 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "     Survived        Age  SibSp  Parch     Fare  female  male  Pclass_1  \\\n",
       "0           0  22.000000      1      0   7.2500       0     1         0   \n",
       "1           1  38.000000      1      0  71.2833       1     0         1   \n",
       "2           1  26.000000      0      0   7.9250       1     0         0   \n",
       "3           1  35.000000      1      0  53.1000       1     0         1   \n",
       "4           0  35.000000      0      0   8.0500       0     1         0   \n",
       "..        ...        ...    ...    ...      ...     ...   ...       ...   \n",
       "886         0  27.000000      0      0  13.0000       0     1         0   \n",
       "887         1  19.000000      0      0  30.0000       1     0         1   \n",
       "888         0  29.642093      1      2  23.4500       1     0         0   \n",
       "889         1  26.000000      0      0  30.0000       0     1         1   \n",
       "890         0  32.000000      0      0   7.7500       0     1         0   \n",
       "\n",
       "     Pclass_2  Pclass_3  Embarked_C  Embarked_Q  Embarked_S  \n",
       "0           0         1           0           0           1  \n",
       "1           0         0           1           0           0  \n",
       "2           0         1           0           0           1  \n",
       "3           0         0           0           0           1  \n",
       "4           0         1           0           0           1  \n",
       "..        ...       ...         ...         ...         ...  \n",
       "886         1         0           0           0           1  \n",
       "887         0         0           0           0           1  \n",
       "888         0         1           0           0           1  \n",
       "889         0         0           1           0           0  \n",
       "890         0         1           0           1           0  \n",
       "\n",
       "[889 rows x 13 columns]"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#One-hot encoding the 'Sex' column\n",
    "train_data = pd.concat([train_data, pd.get_dummies(train_data['Sex'])],axis=1)\n",
    "#One-hot encoding the 'Pclass' column\n",
    "train_data = pd.concat([train_data, pd.get_dummies(train_data['Pclass'], prefix='Pclass')],axis=1)\n",
    "#One-hot encoding the 'Embarked' column\n",
    "train_data = pd.concat([train_data, pd.get_dummies(train_data['Embarked'], prefix='Embarked')],axis=1)\n",
    "\n",
    "#Dropping original columns since they are now encoded\n",
    "train_data = train_data.drop(['Pclass','Sex','Embarked'], axis=1)\n",
    "train_data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that the data has been cleaned, this will be our final dataset! The labels will be the first column (shape 888x1) and the input will be the remaining columns (input vector shape 888x12)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Variables to store final vectors\n",
    "train_input = []\n",
    "train_labels = []\n",
    "for index,row in train_data.iterrows() :\n",
    "    #Append 1st column to labels\n",
    "    train_labels.append(row['Survived'])\n",
    "    #Append remaining columns to input\n",
    "    train_input.append(row[1:])\n",
    "\n",
    "#Convert to NP Arrays\n",
    "train_input = np.asarray(train_input)\n",
    "train_labels = np.asarray(train_labels)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Building the Model\n",
    "For this problem, we will be using a DNN model with the Keras Sequential API. The model architecture will comprise of:\n",
    "1 Input Layer of 12 neurons, 2 Hidden Layers of 10,10 neurons , 1 Dropout Layer and 1 Output Layer of 1 neuron."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_6\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " dense_24 (Dense)            (None, None, 12)          156       \n",
      "                                                                 \n",
      " dense_25 (Dense)            (None, None, 12)          156       \n",
      "                                                                 \n",
      " dense_26 (Dense)            (None, None, 6)           78        \n",
      "                                                                 \n",
      " dense_27 (Dense)            (None, None, 1)           7         \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 397\n",
      "Trainable params: 397\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "np.random.seed(173342) #Set seed for reproducability\n",
    "\n",
    "model = tf.keras.models.Sequential([\n",
    "    #Input Layer\n",
    "    tf.keras.layers.Dense(12,input_shape=(None,12),activation=\"relu\"),\n",
    "    #1st Hidden Layer\n",
    "    tf.keras.layers.Dense(12,activation=\"relu\"),\n",
    "    #2nd Hidden Layer\n",
    "    tf.keras.layers.Dense(6,activation=\"relu\"),\n",
    "    #Output Layer which using sigmoid (because we just have two output classes)\n",
    "    tf.keras.layers.Dense(1,activation=\"sigmoid\")\n",
    "])\n",
    "\n",
    "#print model summary\n",
    "model.summary()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training the Model\n",
    "We will train the model for 200 epochs and a batch size of 32 (default). The optimizer used will be Adam (default learning rate) with the Binary Cross-Entropy Loss function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/200\n",
      "WARNING:tensorflow:Model was constructed with shape (None, None, 12) for input KerasTensor(type_spec=TensorSpec(shape=(None, None, 12), dtype=tf.float32, name='dense_24_input'), name='dense_24_input', description=\"created by layer 'dense_24_input'\"), but it was called on an input with incompatible shape (None, 12).\n",
      "WARNING:tensorflow:Model was constructed with shape (None, None, 12) for input KerasTensor(type_spec=TensorSpec(shape=(None, None, 12), dtype=tf.float32, name='dense_24_input'), name='dense_24_input', description=\"created by layer 'dense_24_input'\"), but it was called on an input with incompatible shape (None, 12).\n",
      " 1/45 [..............................] - ETA: 16s - loss: 7.7369 - accuracy: 0.3000"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-07-21 22:46:56.078330: I tensorflow/core/grappler/optimizers/custom_graph_optimizer_registry.cc:113] Plugin optimizer for device_type GPU is enabled.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "45/45 [==============================] - 1s 9ms/step - loss: 3.8194 - accuracy: 0.4128\n",
      "Epoch 2/200\n",
      "45/45 [==============================] - 0s 7ms/step - loss: 0.8289 - accuracy: 0.6693\n",
      "Epoch 3/200\n",
      "45/45 [==============================] - 0s 7ms/step - loss: 0.6602 - accuracy: 0.6738\n",
      "Epoch 4/200\n",
      "45/45 [==============================] - 0s 7ms/step - loss: 0.6216 - accuracy: 0.6704\n",
      "Epoch 5/200\n",
      "45/45 [==============================] - 0s 7ms/step - loss: 0.6104 - accuracy: 0.6873\n",
      "Epoch 6/200\n",
      "45/45 [==============================] - 0s 7ms/step - loss: 0.6045 - accuracy: 0.6805\n",
      "Epoch 7/200\n",
      "45/45 [==============================] - 0s 7ms/step - loss: 0.5980 - accuracy: 0.6828\n",
      "Epoch 8/200\n",
      "45/45 [==============================] - 0s 8ms/step - loss: 0.5940 - accuracy: 0.6850\n",
      "Epoch 9/200\n",
      "45/45 [==============================] - 0s 7ms/step - loss: 0.5917 - accuracy: 0.6715\n",
      "Epoch 10/200\n",
      "45/45 [==============================] - 0s 7ms/step - loss: 0.5869 - accuracy: 0.6974\n",
      "Epoch 11/200\n",
      "45/45 [==============================] - 0s 7ms/step - loss: 0.5754 - accuracy: 0.6884\n",
      "Epoch 12/200\n",
      "45/45 [==============================] - 0s 7ms/step - loss: 0.5681 - accuracy: 0.6918\n",
      "Epoch 13/200\n",
      "45/45 [==============================] - 0s 7ms/step - loss: 0.5660 - accuracy: 0.7053\n",
      "Epoch 14/200\n",
      "45/45 [==============================] - 0s 7ms/step - loss: 0.5655 - accuracy: 0.6963\n",
      "Epoch 15/200\n",
      "45/45 [==============================] - 0s 7ms/step - loss: 0.5514 - accuracy: 0.7120\n",
      "Epoch 16/200\n",
      "45/45 [==============================] - 0s 7ms/step - loss: 0.5466 - accuracy: 0.7222\n",
      "Epoch 17/200\n",
      "45/45 [==============================] - 0s 7ms/step - loss: 0.5406 - accuracy: 0.7177\n",
      "Epoch 18/200\n",
      "45/45 [==============================] - 0s 7ms/step - loss: 0.5361 - accuracy: 0.7300\n",
      "Epoch 19/200\n",
      "45/45 [==============================] - 0s 7ms/step - loss: 0.5237 - accuracy: 0.7435\n",
      "Epoch 20/200\n",
      "45/45 [==============================] - 0s 7ms/step - loss: 0.5224 - accuracy: 0.7357\n",
      "Epoch 21/200\n",
      "45/45 [==============================] - 0s 7ms/step - loss: 0.5100 - accuracy: 0.7368\n",
      "Epoch 22/200\n",
      "45/45 [==============================] - 0s 7ms/step - loss: 0.5031 - accuracy: 0.7604\n",
      "Epoch 23/200\n",
      "45/45 [==============================] - 0s 7ms/step - loss: 0.4921 - accuracy: 0.7773\n",
      "Epoch 24/200\n",
      "45/45 [==============================] - 0s 7ms/step - loss: 0.4867 - accuracy: 0.7840\n",
      "Epoch 25/200\n",
      "45/45 [==============================] - 0s 7ms/step - loss: 0.4888 - accuracy: 0.7582\n",
      "Epoch 26/200\n",
      "45/45 [==============================] - 0s 7ms/step - loss: 0.4809 - accuracy: 0.7885\n",
      "Epoch 27/200\n",
      "45/45 [==============================] - 0s 7ms/step - loss: 0.4755 - accuracy: 0.7773\n",
      "Epoch 28/200\n",
      "45/45 [==============================] - 0s 7ms/step - loss: 0.4665 - accuracy: 0.7885\n",
      "Epoch 29/200\n",
      "45/45 [==============================] - 0s 7ms/step - loss: 0.4584 - accuracy: 0.7908\n",
      "Epoch 30/200\n",
      "45/45 [==============================] - 0s 7ms/step - loss: 0.4529 - accuracy: 0.7908\n",
      "Epoch 31/200\n",
      "45/45 [==============================] - 0s 7ms/step - loss: 0.4508 - accuracy: 0.7942\n",
      "Epoch 32/200\n",
      "45/45 [==============================] - 0s 7ms/step - loss: 0.4519 - accuracy: 0.7852\n",
      "Epoch 33/200\n",
      "45/45 [==============================] - 0s 7ms/step - loss: 0.4461 - accuracy: 0.8043\n",
      "Epoch 34/200\n",
      "45/45 [==============================] - 0s 7ms/step - loss: 0.4395 - accuracy: 0.7930\n",
      "Epoch 35/200\n",
      "45/45 [==============================] - 0s 7ms/step - loss: 0.4659 - accuracy: 0.7728\n",
      "Epoch 36/200\n",
      "45/45 [==============================] - 0s 7ms/step - loss: 0.4420 - accuracy: 0.8065\n",
      "Epoch 37/200\n",
      "45/45 [==============================] - 0s 7ms/step - loss: 0.4365 - accuracy: 0.8043\n",
      "Epoch 38/200\n",
      "45/45 [==============================] - 0s 7ms/step - loss: 0.4371 - accuracy: 0.8065\n",
      "Epoch 39/200\n",
      "45/45 [==============================] - 0s 7ms/step - loss: 0.4352 - accuracy: 0.8076\n",
      "Epoch 40/200\n",
      "45/45 [==============================] - 0s 7ms/step - loss: 0.4272 - accuracy: 0.8121\n",
      "Epoch 41/200\n",
      "45/45 [==============================] - 0s 7ms/step - loss: 0.4276 - accuracy: 0.8200\n",
      "Epoch 42/200\n",
      "45/45 [==============================] - 0s 7ms/step - loss: 0.4258 - accuracy: 0.8043\n",
      "Epoch 43/200\n",
      "45/45 [==============================] - 0s 7ms/step - loss: 0.4224 - accuracy: 0.8110\n",
      "Epoch 44/200\n",
      "45/45 [==============================] - 0s 7ms/step - loss: 0.4320 - accuracy: 0.8009\n",
      "Epoch 45/200\n",
      "45/45 [==============================] - 0s 7ms/step - loss: 0.4471 - accuracy: 0.8009\n",
      "Epoch 46/200\n",
      "45/45 [==============================] - 0s 7ms/step - loss: 0.4326 - accuracy: 0.8054\n",
      "Epoch 47/200\n",
      "45/45 [==============================] - 0s 7ms/step - loss: 0.4256 - accuracy: 0.8065\n",
      "Epoch 48/200\n",
      "45/45 [==============================] - 0s 7ms/step - loss: 0.4242 - accuracy: 0.8121\n",
      "Epoch 49/200\n",
      "45/45 [==============================] - 0s 7ms/step - loss: 0.4259 - accuracy: 0.8020\n",
      "Epoch 50/200\n",
      "45/45 [==============================] - 0s 7ms/step - loss: 0.4184 - accuracy: 0.8099\n",
      "Epoch 51/200\n",
      "45/45 [==============================] - 0s 7ms/step - loss: 0.4313 - accuracy: 0.8088\n",
      "Epoch 52/200\n",
      "45/45 [==============================] - 0s 7ms/step - loss: 0.4209 - accuracy: 0.8200\n",
      "Epoch 53/200\n",
      "45/45 [==============================] - 0s 7ms/step - loss: 0.4211 - accuracy: 0.8166\n",
      "Epoch 54/200\n",
      "45/45 [==============================] - 0s 7ms/step - loss: 0.4361 - accuracy: 0.8009\n",
      "Epoch 55/200\n",
      "45/45 [==============================] - 0s 7ms/step - loss: 0.4268 - accuracy: 0.8099\n",
      "Epoch 56/200\n",
      "45/45 [==============================] - 0s 7ms/step - loss: 0.4216 - accuracy: 0.8099\n",
      "Epoch 57/200\n",
      "45/45 [==============================] - 0s 7ms/step - loss: 0.4318 - accuracy: 0.8065\n",
      "Epoch 58/200\n",
      "45/45 [==============================] - 0s 7ms/step - loss: 0.4238 - accuracy: 0.8088\n",
      "Epoch 59/200\n",
      "45/45 [==============================] - 0s 7ms/step - loss: 0.4199 - accuracy: 0.8110\n",
      "Epoch 60/200\n",
      "45/45 [==============================] - 0s 7ms/step - loss: 0.4155 - accuracy: 0.8110\n",
      "Epoch 61/200\n",
      "45/45 [==============================] - 0s 7ms/step - loss: 0.4285 - accuracy: 0.8076\n",
      "Epoch 62/200\n",
      "45/45 [==============================] - 0s 7ms/step - loss: 0.4300 - accuracy: 0.8088\n",
      "Epoch 63/200\n",
      "45/45 [==============================] - 0s 7ms/step - loss: 0.4201 - accuracy: 0.8189\n",
      "Epoch 64/200\n",
      "45/45 [==============================] - 0s 7ms/step - loss: 0.4233 - accuracy: 0.8110\n",
      "Epoch 65/200\n",
      "45/45 [==============================] - 0s 7ms/step - loss: 0.4220 - accuracy: 0.8178\n",
      "Epoch 66/200\n",
      "45/45 [==============================] - 0s 7ms/step - loss: 0.4206 - accuracy: 0.8155\n",
      "Epoch 67/200\n",
      "45/45 [==============================] - 0s 7ms/step - loss: 0.4374 - accuracy: 0.8054\n",
      "Epoch 68/200\n",
      "45/45 [==============================] - 0s 7ms/step - loss: 0.4265 - accuracy: 0.8054\n",
      "Epoch 69/200\n",
      "45/45 [==============================] - 0s 7ms/step - loss: 0.4185 - accuracy: 0.8099\n",
      "Epoch 70/200\n",
      "45/45 [==============================] - 0s 7ms/step - loss: 0.4142 - accuracy: 0.8189\n",
      "Epoch 71/200\n",
      "45/45 [==============================] - 0s 7ms/step - loss: 0.4145 - accuracy: 0.8166\n",
      "Epoch 72/200\n",
      "45/45 [==============================] - 0s 7ms/step - loss: 0.4157 - accuracy: 0.8133\n",
      "Epoch 73/200\n",
      "45/45 [==============================] - 0s 7ms/step - loss: 0.4321 - accuracy: 0.8020\n",
      "Epoch 74/200\n",
      "45/45 [==============================] - 0s 7ms/step - loss: 0.4254 - accuracy: 0.8088\n",
      "Epoch 75/200\n",
      "45/45 [==============================] - 0s 7ms/step - loss: 0.4154 - accuracy: 0.8211\n",
      "Epoch 76/200\n",
      "45/45 [==============================] - 0s 7ms/step - loss: 0.4105 - accuracy: 0.8223\n",
      "Epoch 77/200\n",
      "45/45 [==============================] - 0s 7ms/step - loss: 0.4139 - accuracy: 0.8166\n",
      "Epoch 78/200\n",
      "45/45 [==============================] - 0s 7ms/step - loss: 0.4114 - accuracy: 0.8234\n",
      "Epoch 79/200\n",
      "45/45 [==============================] - 0s 7ms/step - loss: 0.4078 - accuracy: 0.8178\n",
      "Epoch 80/200\n",
      "45/45 [==============================] - 0s 7ms/step - loss: 0.4146 - accuracy: 0.8178\n",
      "Epoch 81/200\n",
      "45/45 [==============================] - 0s 7ms/step - loss: 0.4164 - accuracy: 0.8256\n",
      "Epoch 82/200\n",
      "45/45 [==============================] - 0s 7ms/step - loss: 0.4084 - accuracy: 0.8245\n",
      "Epoch 83/200\n",
      "45/45 [==============================] - 0s 7ms/step - loss: 0.4071 - accuracy: 0.8223\n",
      "Epoch 84/200\n",
      "45/45 [==============================] - 0s 7ms/step - loss: 0.4102 - accuracy: 0.8245\n",
      "Epoch 85/200\n",
      "45/45 [==============================] - 0s 7ms/step - loss: 0.4201 - accuracy: 0.8178\n",
      "Epoch 86/200\n",
      "45/45 [==============================] - 0s 7ms/step - loss: 0.4127 - accuracy: 0.8178\n",
      "Epoch 87/200\n",
      "45/45 [==============================] - 0s 7ms/step - loss: 0.4141 - accuracy: 0.8189\n",
      "Epoch 88/200\n",
      "45/45 [==============================] - 0s 7ms/step - loss: 0.4076 - accuracy: 0.8234\n",
      "Epoch 89/200\n",
      "45/45 [==============================] - 0s 7ms/step - loss: 0.4117 - accuracy: 0.8223\n",
      "Epoch 90/200\n",
      "45/45 [==============================] - 0s 7ms/step - loss: 0.4042 - accuracy: 0.8234\n",
      "Epoch 91/200\n",
      "45/45 [==============================] - 0s 7ms/step - loss: 0.4176 - accuracy: 0.8099\n",
      "Epoch 92/200\n",
      "45/45 [==============================] - 0s 7ms/step - loss: 0.4072 - accuracy: 0.8178\n",
      "Epoch 93/200\n",
      "45/45 [==============================] - 0s 7ms/step - loss: 0.4189 - accuracy: 0.8189\n",
      "Epoch 94/200\n",
      "45/45 [==============================] - 0s 7ms/step - loss: 0.4085 - accuracy: 0.8279\n",
      "Epoch 95/200\n",
      "45/45 [==============================] - 0s 7ms/step - loss: 0.4132 - accuracy: 0.8200\n",
      "Epoch 96/200\n",
      "45/45 [==============================] - 0s 7ms/step - loss: 0.4098 - accuracy: 0.8256\n",
      "Epoch 97/200\n",
      "45/45 [==============================] - 0s 7ms/step - loss: 0.4106 - accuracy: 0.8335\n",
      "Epoch 98/200\n",
      "45/45 [==============================] - 0s 7ms/step - loss: 0.4136 - accuracy: 0.8245\n",
      "Epoch 99/200\n",
      "45/45 [==============================] - 0s 7ms/step - loss: 0.4107 - accuracy: 0.8301\n",
      "Epoch 100/200\n",
      "45/45 [==============================] - 0s 7ms/step - loss: 0.4178 - accuracy: 0.8178\n",
      "Epoch 101/200\n",
      "45/45 [==============================] - 0s 7ms/step - loss: 0.4168 - accuracy: 0.8256\n",
      "Epoch 102/200\n",
      "45/45 [==============================] - 0s 7ms/step - loss: 0.4038 - accuracy: 0.8324\n",
      "Epoch 103/200\n",
      "45/45 [==============================] - 0s 7ms/step - loss: 0.4120 - accuracy: 0.8245\n",
      "Epoch 104/200\n",
      "45/45 [==============================] - 0s 7ms/step - loss: 0.4028 - accuracy: 0.8256\n",
      "Epoch 105/200\n",
      "45/45 [==============================] - 0s 7ms/step - loss: 0.4060 - accuracy: 0.8200\n",
      "Epoch 106/200\n",
      "45/45 [==============================] - 0s 7ms/step - loss: 0.4052 - accuracy: 0.8211\n",
      "Epoch 107/200\n",
      "45/45 [==============================] - 0s 7ms/step - loss: 0.4051 - accuracy: 0.8268\n",
      "Epoch 108/200\n",
      "45/45 [==============================] - 0s 7ms/step - loss: 0.4176 - accuracy: 0.8223\n",
      "Epoch 109/200\n",
      "45/45 [==============================] - 0s 7ms/step - loss: 0.4052 - accuracy: 0.8279\n",
      "Epoch 110/200\n",
      "45/45 [==============================] - 0s 7ms/step - loss: 0.4179 - accuracy: 0.8200\n",
      "Epoch 111/200\n",
      "45/45 [==============================] - 0s 7ms/step - loss: 0.4201 - accuracy: 0.8189\n",
      "Epoch 112/200\n",
      "45/45 [==============================] - 0s 7ms/step - loss: 0.4079 - accuracy: 0.8256\n",
      "Epoch 113/200\n",
      "45/45 [==============================] - 0s 7ms/step - loss: 0.4032 - accuracy: 0.8234\n",
      "Epoch 114/200\n",
      "45/45 [==============================] - 0s 7ms/step - loss: 0.4059 - accuracy: 0.8256\n",
      "Epoch 115/200\n",
      "45/45 [==============================] - 0s 7ms/step - loss: 0.4066 - accuracy: 0.8301\n",
      "Epoch 116/200\n",
      "45/45 [==============================] - 0s 7ms/step - loss: 0.4092 - accuracy: 0.8245\n",
      "Epoch 117/200\n",
      "45/45 [==============================] - 0s 7ms/step - loss: 0.4107 - accuracy: 0.8256\n",
      "Epoch 118/200\n",
      "45/45 [==============================] - 0s 7ms/step - loss: 0.4031 - accuracy: 0.8245\n",
      "Epoch 119/200\n",
      "45/45 [==============================] - 0s 7ms/step - loss: 0.4065 - accuracy: 0.8200\n",
      "Epoch 120/200\n",
      "45/45 [==============================] - 0s 7ms/step - loss: 0.4068 - accuracy: 0.8324\n",
      "Epoch 121/200\n",
      "45/45 [==============================] - 0s 7ms/step - loss: 0.4018 - accuracy: 0.8301\n",
      "Epoch 122/200\n",
      "45/45 [==============================] - 0s 7ms/step - loss: 0.4089 - accuracy: 0.8200\n",
      "Epoch 123/200\n",
      "45/45 [==============================] - 0s 7ms/step - loss: 0.4049 - accuracy: 0.8256\n",
      "Epoch 124/200\n",
      "45/45 [==============================] - 0s 7ms/step - loss: 0.4093 - accuracy: 0.8223\n",
      "Epoch 125/200\n",
      "45/45 [==============================] - 0s 7ms/step - loss: 0.4014 - accuracy: 0.8290\n",
      "Epoch 126/200\n",
      "45/45 [==============================] - 0s 7ms/step - loss: 0.4068 - accuracy: 0.8290\n",
      "Epoch 127/200\n",
      "45/45 [==============================] - 0s 7ms/step - loss: 0.3986 - accuracy: 0.8313\n",
      "Epoch 128/200\n",
      "45/45 [==============================] - 0s 7ms/step - loss: 0.4047 - accuracy: 0.8256\n",
      "Epoch 129/200\n",
      "45/45 [==============================] - 0s 7ms/step - loss: 0.4018 - accuracy: 0.8301\n",
      "Epoch 130/200\n",
      "45/45 [==============================] - 0s 7ms/step - loss: 0.4004 - accuracy: 0.8290\n",
      "Epoch 131/200\n",
      "45/45 [==============================] - 0s 7ms/step - loss: 0.4042 - accuracy: 0.8234\n",
      "Epoch 132/200\n",
      "45/45 [==============================] - 0s 7ms/step - loss: 0.4044 - accuracy: 0.8279\n",
      "Epoch 133/200\n",
      "45/45 [==============================] - 0s 7ms/step - loss: 0.4044 - accuracy: 0.8234\n",
      "Epoch 134/200\n",
      "45/45 [==============================] - 0s 7ms/step - loss: 0.4114 - accuracy: 0.8200\n",
      "Epoch 135/200\n",
      "45/45 [==============================] - 0s 7ms/step - loss: 0.4086 - accuracy: 0.8245\n",
      "Epoch 136/200\n",
      "45/45 [==============================] - 0s 7ms/step - loss: 0.3954 - accuracy: 0.8290\n",
      "Epoch 137/200\n",
      "45/45 [==============================] - 0s 7ms/step - loss: 0.4042 - accuracy: 0.8279\n",
      "Epoch 138/200\n",
      "45/45 [==============================] - 0s 7ms/step - loss: 0.4101 - accuracy: 0.8200\n",
      "Epoch 139/200\n",
      "45/45 [==============================] - 0s 7ms/step - loss: 0.4052 - accuracy: 0.8268\n",
      "Epoch 140/200\n",
      "45/45 [==============================] - 0s 7ms/step - loss: 0.3992 - accuracy: 0.8313\n",
      "Epoch 141/200\n",
      "45/45 [==============================] - 0s 7ms/step - loss: 0.4011 - accuracy: 0.8324\n",
      "Epoch 142/200\n",
      "45/45 [==============================] - 0s 7ms/step - loss: 0.4073 - accuracy: 0.8223\n",
      "Epoch 143/200\n",
      "45/45 [==============================] - 0s 7ms/step - loss: 0.4013 - accuracy: 0.8380\n",
      "Epoch 144/200\n",
      "45/45 [==============================] - 0s 7ms/step - loss: 0.4070 - accuracy: 0.8290\n",
      "Epoch 145/200\n",
      "45/45 [==============================] - 0s 7ms/step - loss: 0.4006 - accuracy: 0.8234\n",
      "Epoch 146/200\n",
      "45/45 [==============================] - 0s 7ms/step - loss: 0.4026 - accuracy: 0.8335\n",
      "Epoch 147/200\n",
      "45/45 [==============================] - 0s 7ms/step - loss: 0.4138 - accuracy: 0.8313\n",
      "Epoch 148/200\n",
      "45/45 [==============================] - 0s 7ms/step - loss: 0.4003 - accuracy: 0.8335\n",
      "Epoch 149/200\n",
      "45/45 [==============================] - 0s 7ms/step - loss: 0.3953 - accuracy: 0.8380\n",
      "Epoch 150/200\n",
      "45/45 [==============================] - 0s 7ms/step - loss: 0.3997 - accuracy: 0.8200\n",
      "Epoch 151/200\n",
      "45/45 [==============================] - 0s 7ms/step - loss: 0.3970 - accuracy: 0.8335\n",
      "Epoch 152/200\n",
      "45/45 [==============================] - 0s 7ms/step - loss: 0.3985 - accuracy: 0.8301\n",
      "Epoch 153/200\n",
      "45/45 [==============================] - 0s 7ms/step - loss: 0.3986 - accuracy: 0.8279\n",
      "Epoch 154/200\n",
      "45/45 [==============================] - 0s 7ms/step - loss: 0.4022 - accuracy: 0.8301\n",
      "Epoch 155/200\n",
      "45/45 [==============================] - 0s 7ms/step - loss: 0.3944 - accuracy: 0.8380\n",
      "Epoch 156/200\n",
      "45/45 [==============================] - 0s 7ms/step - loss: 0.3953 - accuracy: 0.8268\n",
      "Epoch 157/200\n",
      "45/45 [==============================] - 0s 7ms/step - loss: 0.3989 - accuracy: 0.8290\n",
      "Epoch 158/200\n",
      "45/45 [==============================] - 0s 7ms/step - loss: 0.3974 - accuracy: 0.8268\n",
      "Epoch 159/200\n",
      "45/45 [==============================] - 0s 7ms/step - loss: 0.3936 - accuracy: 0.8313\n",
      "Epoch 160/200\n",
      "45/45 [==============================] - 0s 7ms/step - loss: 0.4007 - accuracy: 0.8178\n",
      "Epoch 161/200\n",
      "45/45 [==============================] - 0s 7ms/step - loss: 0.3994 - accuracy: 0.8301\n",
      "Epoch 162/200\n",
      "45/45 [==============================] - 0s 7ms/step - loss: 0.4021 - accuracy: 0.8268\n",
      "Epoch 163/200\n",
      "45/45 [==============================] - 0s 7ms/step - loss: 0.4057 - accuracy: 0.8245\n",
      "Epoch 164/200\n",
      "45/45 [==============================] - 0s 7ms/step - loss: 0.3970 - accuracy: 0.8324\n",
      "Epoch 165/200\n",
      "45/45 [==============================] - 0s 7ms/step - loss: 0.3969 - accuracy: 0.8346\n",
      "Epoch 166/200\n",
      "45/45 [==============================] - 0s 7ms/step - loss: 0.3934 - accuracy: 0.8369\n",
      "Epoch 167/200\n",
      "45/45 [==============================] - 0s 7ms/step - loss: 0.4141 - accuracy: 0.8256\n",
      "Epoch 168/200\n",
      "45/45 [==============================] - 0s 7ms/step - loss: 0.4082 - accuracy: 0.8155\n",
      "Epoch 169/200\n",
      "45/45 [==============================] - 0s 7ms/step - loss: 0.3978 - accuracy: 0.8301\n",
      "Epoch 170/200\n",
      "45/45 [==============================] - 0s 7ms/step - loss: 0.3947 - accuracy: 0.8256\n",
      "Epoch 171/200\n",
      "45/45 [==============================] - 0s 7ms/step - loss: 0.4014 - accuracy: 0.8324\n",
      "Epoch 172/200\n",
      "45/45 [==============================] - 0s 7ms/step - loss: 0.3932 - accuracy: 0.8324\n",
      "Epoch 173/200\n",
      "45/45 [==============================] - 0s 7ms/step - loss: 0.3994 - accuracy: 0.8313\n",
      "Epoch 174/200\n",
      "45/45 [==============================] - 0s 7ms/step - loss: 0.4018 - accuracy: 0.8245\n",
      "Epoch 175/200\n",
      "45/45 [==============================] - 0s 7ms/step - loss: 0.3935 - accuracy: 0.8256\n",
      "Epoch 176/200\n",
      "45/45 [==============================] - 0s 7ms/step - loss: 0.3905 - accuracy: 0.8335\n",
      "Epoch 177/200\n",
      "45/45 [==============================] - 0s 7ms/step - loss: 0.3930 - accuracy: 0.8324\n",
      "Epoch 178/200\n",
      "45/45 [==============================] - 0s 7ms/step - loss: 0.4037 - accuracy: 0.8279\n",
      "Epoch 179/200\n",
      "45/45 [==============================] - 0s 7ms/step - loss: 0.3933 - accuracy: 0.8313\n",
      "Epoch 180/200\n",
      "45/45 [==============================] - 0s 7ms/step - loss: 0.3918 - accuracy: 0.8369\n",
      "Epoch 181/200\n",
      "45/45 [==============================] - 0s 7ms/step - loss: 0.3895 - accuracy: 0.8313\n",
      "Epoch 182/200\n",
      "45/45 [==============================] - 0s 7ms/step - loss: 0.3970 - accuracy: 0.8380\n",
      "Epoch 183/200\n",
      "45/45 [==============================] - 0s 7ms/step - loss: 0.3955 - accuracy: 0.8313\n",
      "Epoch 184/200\n",
      "45/45 [==============================] - 0s 7ms/step - loss: 0.3947 - accuracy: 0.8358\n",
      "Epoch 185/200\n",
      "45/45 [==============================] - 0s 7ms/step - loss: 0.3892 - accuracy: 0.8403\n",
      "Epoch 186/200\n",
      "45/45 [==============================] - 0s 7ms/step - loss: 0.3967 - accuracy: 0.8335\n",
      "Epoch 187/200\n",
      "45/45 [==============================] - 0s 7ms/step - loss: 0.3964 - accuracy: 0.8290\n",
      "Epoch 188/200\n",
      "45/45 [==============================] - 0s 7ms/step - loss: 0.3878 - accuracy: 0.8346\n",
      "Epoch 189/200\n",
      "45/45 [==============================] - 0s 7ms/step - loss: 0.3902 - accuracy: 0.8335\n",
      "Epoch 190/200\n",
      "45/45 [==============================] - 0s 7ms/step - loss: 0.3907 - accuracy: 0.8290\n",
      "Epoch 191/200\n",
      "45/45 [==============================] - 0s 7ms/step - loss: 0.3917 - accuracy: 0.8324\n",
      "Epoch 192/200\n",
      "45/45 [==============================] - 0s 7ms/step - loss: 0.3977 - accuracy: 0.8279\n",
      "Epoch 193/200\n",
      "45/45 [==============================] - 0s 7ms/step - loss: 0.4015 - accuracy: 0.8256\n",
      "Epoch 194/200\n",
      "45/45 [==============================] - 0s 7ms/step - loss: 0.3991 - accuracy: 0.8279\n",
      "Epoch 195/200\n",
      "45/45 [==============================] - 0s 7ms/step - loss: 0.3993 - accuracy: 0.8358\n",
      "Epoch 196/200\n",
      "45/45 [==============================] - 0s 7ms/step - loss: 0.4147 - accuracy: 0.8088\n",
      "Epoch 197/200\n",
      "45/45 [==============================] - 0s 7ms/step - loss: 0.3975 - accuracy: 0.8313\n",
      "Epoch 198/200\n",
      "45/45 [==============================] - 0s 7ms/step - loss: 0.3822 - accuracy: 0.8380\n",
      "Epoch 199/200\n",
      "45/45 [==============================] - 0s 7ms/step - loss: 0.3904 - accuracy: 0.8346\n",
      "Epoch 200/200\n",
      "45/45 [==============================] - 0s 7ms/step - loss: 0.4065 - accuracy: 0.8223\n"
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "optimizer = tf.keras.optimizers.Adam()\n",
    "lossFn = tf.keras.losses.BinaryCrossentropy()\n",
    "\n",
    "#Compile the model\n",
    "model.compile(loss=lossFn,optimizer=optimizer,metrics=[\"accuracy\"])\n",
    "\n",
    "#Training the model\n",
    "history = model.fit(train_input,train_labels,epochs=200,batch_size=20)\n",
    "\n",
    "#Plot the results\n",
    "acc=history.history['accuracy']\n",
    "epochs=range(len(acc)) # Get number of epochs\n",
    "\n",
    "plt.plot(epochs, acc, 'r')\n",
    "plt.title('Accuracy Curve')\n",
    "plt.show()\n",
    "print(\"\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.10.5 ('tf_Dev')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.5"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "4da46fc40f1915703b38afd3791e6a316bca5498ac50c4dc99c37cd966af4760"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
